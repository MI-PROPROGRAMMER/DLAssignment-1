import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import keras
from google.colab import files
uploaded = files.upload()
for keys in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=keys, length=len(uploaded[keys])))
data = pd.read_csv("compresive_strength_concrete.csv")
data.head()
print(data.shape)
data.info()

pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)


data.isnull().sum()
train_data = data.iloc[0:515]
train_targets = data.iloc[0:515]

val_data = data.iloc[515:721]
val_targets = data.iloc[515:721]

test_data = data.iloc[721:]
test_targets =data.iloc[721:]

print(train_data.shape)
print(train_targets.shape)

print(val_data.shape)
print(val_targets.shape)

print(test_data.shape)
print(test_targets.shape)
mean = train_data.mean()
std = train_data.std()

train_data -= mean
train_data /= std

val_data -= mean
val_data /= std

test_data -= mean
test_data /= std

model = models.Sequential()
    
model.add(layers.Dense(10, activation='relu', input_shape=(train_data.shape[1],)))  
model.add(layers.Dense(8, activation='relu'))
model.add(layers.Dense(6, activation='relu'))
model.add(layers.Dense(1))

model.compile(optimizer='Adam', loss='mse', metrics=['mae'])

history = model.fit(train_data, train_targets, validation_data = (val_data, val_targets), epochs=100, verbose=1)

predictions = model.predict(test_data)
predictions.shape
predictions = predictions.reshape(309)
y_pred = model.predict(train_data)
print(y_pred)

model.evaluate(train_data, train_targets)
model.evaluate(test_data, test_targets)












